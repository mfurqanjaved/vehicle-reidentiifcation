{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwGJaqPT62uK",
        "outputId": "2d9e98ab-67f3-4277-ead4-d0bcbd9e1d79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the path of the password-protected zip file you want to extract\n",
        "zip_file_path = \"/content/drive/MyDrive/data/VeRi-EEEM071 (2).zip\"\n",
        "\n",
        "# Specify the password for the zip file\n",
        "password = \"sUrreY_eeEm071_VeRi\"\n",
        "\n",
        "# Create a ZipFile object and extract its contents to a specified directory\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/drive/MyDrive/new_data\", pwd=password.encode('utf-8'))"
      ],
      "metadata": {
        "id": "wPriHkeV7m94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Google Drive API credentials are set up and a connection to Google Drive is established. The path to a folder containing XML files is specified, as well as the path to the folder where the resulting CSV files will be saved. The code then iterates over each XML file in the specified folder. For each XML file, it parses the file and creates a corresponding CSV file. The XML headers are extracted and written as the headers in the CSV file. The XML data is extracted and written as rows in the CSV file. Finally, the generated CSV file is uploaded to Google Drive and the ID of the uploaded file is printed."
      ],
      "metadata": {
        "id": "DldPDXWs4UVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import csv\n",
        "import xml.etree.ElementTree as ET\n",
        "from google.colab import drive\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.errors import HttpError\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Set up the Google Drive API credentials\n",
        "auth.authenticate_user()\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "drive_service = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "# # Mount Google Drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to the folder containing the XML files in Google Drive\n",
        "xml_folder_path = '/content/drive/MyDrive/new_data/train_label.xml'\n",
        "\n",
        "# Set the path to the folder where the CSV files will be saved in Google Drive\n",
        "csv_folder_path = '/content/drive/MyDrive/new_data/VeRi/train_label.csv'\n",
        "\n",
        "# Iterate over all XML files in the specified folder\n",
        "for filename in os.listdir(xml_folder_path):\n",
        "  if filename.endswith('.xml'):\n",
        "    # Parse the XML file\n",
        "    xml_tree = ET.parse(xml_folder_path + filename)\n",
        "    root = xml_tree.getroot()\n",
        "\n",
        "    # Create a CSV file with the same name as the XML file\n",
        "    csv_filename = os.path.splitext(filename)[0] + '.csv'\n",
        "    csv_file = open(csv_folder_path + csv_filename, 'w', newline='')\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # Write the headers to the CSV file\n",
        "    headers = []\n",
        "    for child in root[0]:\n",
        "      headers.append(child.tag)\n",
        "    csv_writer.writerow(headers)\n",
        "\n",
        "    # Write the data to the CSV file\n",
        "    for child in root:\n",
        "      row = []\n",
        "      for element in child:\n",
        "        row.append(element.text)\n",
        "      csv_writer.writerow(row)\n",
        "\n",
        "    # Close the CSV file\n",
        "    csv_file.close()\n",
        "\n",
        "    # Upload the CSV file to Google Drive\n",
        "    file_metadata = {'name': csv_filename, 'parents': [drive_service.files().get(fileId='root').execute()['id']]}\n",
        "    media = {'mimeType': 'text/csv', 'body': open(csv_folder_path + csv_filename, 'rb')}\n",
        "    file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
        "    print('CSV file ID: %s' % file.get('id'))"
      ],
      "metadata": {
        "id": "P5zD2QB0rpXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "metadata": {
        "id": "coWEcJ8TGToy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a transformation pipeline for images. It resizes the images to 224x224 pixels, converts them to tensors, and applies normalization to bring the pixel values within the range of -1 to 1."
      ],
      "metadata": {
        "id": "T_BtgIvU4fyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transforms to apply to the images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    \n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])"
      ],
      "metadata": {
        "id": "D54P3CswGWHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "with open('/content/drive/MyDrive/new_data/VeRi/train_label.xml','r') as f:\n",
        "   ef = ET.fromstring(f.read())"
      ],
      "metadata": {
        "id": "jtzTs-_xWlkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining a custom dataset class called CustomDataset that extends the Dataset class from PyTorch. It takes a root directory, a labels file, and an optional transformation as input. The dataset's length is determined by the number of labels. The __getitem__ method retrieves an image and its corresponding label by parsing the labels file. The image is loaded, converted to RGB, and transformed if the transform is specified. Finally, the method returns the transformed image and its label. The _parse_labels method reads the labels file, parses the XML content, and returns a list of dictionaries containing image names and labels."
      ],
      "metadata": {
        "id": "6z7NbHvR4omq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self._parse_labels(labels_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.root_dir, self.labels[idx]['img_name'])\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]['label']\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def _parse_labels(self, labels_file):\n",
        "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "          tree = ET.parse(f.read())\n",
        "          root = tree.getroot()\n",
        "          labels = []\n",
        "        for child in root:\n",
        "            if child.tag != 'image':\n",
        "                continue\n",
        "            img_name = child.attrib['name']\n",
        "            label = int(child.find('label').text)\n",
        "            labels.append({'img_name': img_name, 'label': label})\n",
        "        return labels"
      ],
      "metadata": {
        "id": "_pnjM-UGSHSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A dataset and dataloader are created using the CustomDataset class. The root_dir specifies the directory where the images are located, and the labels_file contains the corresponding labels for each image. The dataset is transformed using the defined transform object, and the dataloader is set to batch size 256, shuffling the data and using 4 workers for parallel data loading."
      ],
      "metadata": {
        "id": "gslizHgM47qB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model and Training Setup:\n",
        "\n",
        "\n",
        "*   A ResNet-18 model is created and customized for classification by replacing the last fully connected layer.\n",
        "*   The model is configured with an output size of 10, assuming 10 classes.\n",
        "\n",
        "\n",
        "*   The loss function is defined as cross-entropy.\n",
        "*   The optimizer used is stochastic gradient descent (SGD) with a learning rate of 0.001 and momentum of 0.9.\n",
        "\n",
        "### Training Process:\n",
        "\n",
        " \n",
        "\n",
        "*   The model is trained for 30 epochs.\n",
        "*   For each epoch, the running loss is initialized to 0.0.\n",
        "*   The training data is iterated through in batches.\n",
        "*   Inputs and labels are retrieved for each batch.\n",
        "*   Optimizer gradients are set to zero.\n",
        "*   List item\n",
        "*   The forward pass is performed to obtain the outputs.\n",
        "*   The loss is calculated using the cross-entropy loss function.\n",
        "*   Backpropagation is applied, and the optimizer updates the model's parameters.\n",
        "*   The running loss is accumulated.\n",
        "*   Every 10 mini-batches, the average loss is printed.\n",
        "*   After training, the message \"Finished Training\" is displayed.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_dpjtRvi4_2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset and dataloader\n",
        "root_dir = '/content/drive/MyDrive/new_data/VeRi/image_train'\n",
        "labels_file = '/content/drive/MyDrive/new_data/VeRi/train_label.xml'\n",
        "dataset = CustomDataset(root_dir=root_dir, labels_file=labels_file, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)\n",
        "\n",
        "# create model and define loss function and optimizer\n",
        "model = models.resnet18()\n",
        "model.fc = nn.Linear(512, 10)  # assuming 10 classes\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# train model\n",
        "for epoch in range(30):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get inputs and labels\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 10))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# evaluate model on test data\n",
        "# define test dataset and dataloader in the same way as train data\n",
        "\n",
        "# set model to evaluation mode\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "X7OCtqZMj2eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing Resnet 18"
      ],
      "metadata": {
        "id": "3HbgQj5gWEXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define transformations to be applied to each image\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# define custom test dataset class\n",
        "class CustomTestDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self._parse_labels(labels_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.labels[idx]['img_name']\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    \n",
        "    def _parse_labels(self, labels_file):\n",
        "        tree = ET.parse(labels_file)\n",
        "        root = tree.getroot()\n",
        "        labels = []\n",
        "        for child in root:\n",
        "            if child.tag != 'image':\n",
        "                continue\n",
        "            img_name = child.attrib['name']\n",
        "            label = int(child.find('label').text)\n",
        "            labels.append({'img_name': img_name, 'label': label})\n",
        "        return labels\n",
        "\n",
        "# create test dataset and dataloader\n",
        "test_root_dir = '/content/drive/MyDrive/new_data/VeRi/image_test'\n",
        "test_labels_file = '/content/drive/MyDrive/new_data/VeRi/test_label.xml'\n",
        "test_dataset = CustomTestDataset(test_root_dir, test_labels_file, transform=test_transform)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# define variables to keep track of total and correct predictions\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# iterate over test data\n",
        "with torch.no_grad():\n",
        "    for data in test_dataloader:\n",
        "        # get inputs and labels\n",
        "        inputs, labels = data\n",
        "\n",
        "        # forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # get predicted labels\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # update total and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# calculate accuracy on test data\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy on test data: {:.2f}%'.format(accuracy))"
      ],
      "metadata": {
        "id": "jy0mTUOtUHh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fitting vgg 16"
      ],
      "metadata": {
        "id": "olJ7EepZWpih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define transformations to be applied to each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# define custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self._parse_labels(labels_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.labels[idx]['img_name']\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    \n",
        "    def _parse_labels(self, labels_file):\n",
        "        tree = ET.parse(labels_file)\n",
        "        root = tree.getroot()\n",
        "        labels = []\n",
        "        for child in root:\n",
        "            if child.tag != 'image':\n",
        "                continue\n",
        "            img_name = child.attrib['name']\n",
        "            label = int(child.find('label').text)\n",
        "            labels.append({'img_name': img_name, 'label': label})\n",
        "        return labels\n",
        "\n",
        "# create dataset and dataloader\n",
        "root_dir = '/content/drive/MyDrive/new_data/VeRi/train_label.xml'\n",
        "labels_file = '/content/drive/MyDrive/new_data/VeRi/test_label.xml'\n",
        "dataset = CustomDataset(root_dir, labels_file, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)\n",
        "\n",
        "# create model and define loss function and optimizer\n",
        "model = models.vgg16_bn()\n",
        "model.classifier[6] = nn.Linear(4096, 10)  # assuming 10 classes\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# train model\n",
        "for epoch in range(30):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get inputs and labels\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 10))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "qFcT4olnWrtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## testing vgg 16"
      ],
      "metadata": {
        "id": "8HYGz3mrW5Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, labels_file, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.labels = self._parse_labels(labels_file)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.labels[idx]['img_name']\n",
        "        img_path = os.path.join(self.root_dir, img_name)\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]['label']\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "    \n",
        "    def _parse_labels(self, labels_file):\n",
        "        tree = ET.parse(labels_file)\n",
        "        root = tree.getroot()\n",
        "        labels = []\n",
        "        for child in root:\n",
        "            if child.tag != 'image':\n",
        "                continue\n",
        "            img_name = child.attrib['name']\n",
        "            label = int(child.find('label').text)\n",
        "            labels.append({'img_name': img_name, 'label': label})\n",
        "        return labels\n",
        "\n",
        "# create dataset and dataloader\n",
        "root_dir = '/content/drive/MyDrive/new_data/VeRi/image_test'\n",
        "labels_file = '/content/drive/MyDrive/new_data/VeRi/test_label.xml'\n",
        "dataset = CustomDataset(root_dir, labels_file, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4)\n",
        "\n",
        "# create model and define loss function and optimizer\n",
        "model = models.vgg16()\n",
        "model.classifier[-1] = nn.Linear(4096, 10)  # assuming 10 classes\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# train model\n",
        "for epoch in range(30):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # get inputs and labels\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:    # print every 10 mini-batches\n",
        "            print('[Epoch %d, Batch %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 10))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# evaluate model on test data\n",
        "# define test dataset and dataloader in the same way as train data\n",
        "\n",
        "# set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# define variables to keep track of total and correct predictions\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "# iterate over test data\n",
        "for data in test_dataloader:\n",
        "    # get inputs and labels\n",
        "    inputs, labels = data\n",
        "\n",
        "    # forward pass\n",
        "    outputs = model(inputs)\n",
        "\n",
        "\n",
        "    # update total and correct predictions\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "# calculate accuracy on test data\n",
        "accuracy = 100 * correct / total\n",
        "print('Accuracy on test data: {:.2f}%'.format(accuracy))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GdOwR--2XB_B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}